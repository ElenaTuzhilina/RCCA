#' control the sparsity on a group level of the canonical coefficients \eqn{\alpha} and \eqn{\beta}.
#'
#'
#' @param X a rectangular \eqn{n x p} matrix containing \eqn{n} observations of random vector \eqn{x}.
#' @param Y a rectangular \eqn{n x q} matrix containing \eqn{n} observations of random vector \eqn{y}.
#' @param group1 an integer valued vector representing the group assignment of \eqn{\alpha} coefficients. By default \code{group1 = rep(1, ncol(X))}, i.e. we include all \eqn{\alpha} coefficients in the same group.
#' @param group2 an integer valued vector representing the group assignment of \eqn{\beta} coefficients. By default \code{group2 = rep(1, ncol(Y))}, i.e. we include all \eqn{\beta} coefficients in the same group.
#' @param lambda1 a non-negative penalty factor used for controlling the within variation of \eqn{\alpha} coefficients. By default \code{lambda1 = 0}, i.e. no regularization is imposed. Increasing \code{lambda1} shrinks each coefficient toward it's group mean.
#' @param lambda2 a non-negative penalty factor used for controlling the within variation of \eqn{\beta} coefficients. By default \code{lambda2 = 0}, i.e. no regularization is imposed. Increasing \code{lambda2} shrinks each coefficient toward it's group mean.
#' @param mu1 a non-negative penalty factor used for controlling the between variation of \eqn{\alpha} coefficients. By default \code{mu1 = 0}, i.e. no regularization is imposed. Increasing \code{mu1} shrinks each group mean toward zero.
#' @param mu2 a non-negative penalty factor used for controlling the between variation of \eqn{\beta} coefficients. By default \code{mu2 = 0}, i.e. no regularization is imposed. Increasing \code{mu2} shrinks each group mean toward zero.
#' @return A list containing the PCMS problem solution:
#' \itemize{
#'   \item \code{n.comp} -- the number of computed canonical components, i.e. \eqn{k = min(p, q)}.
#'   \item \code{cors} -- the resulting \eqn{k} canonical correlations.
#'   \item \code{mod.cors} -- the resulting \eqn{k} values of modified canonical correlation.
#'   \item \code{x.coefs} -- \eqn{p x k} matrix representing \eqn{k} canonical coefficient vectors \eqn{\alpha[1]}, ..., \eqn{\alpha[k]}.
#'   \item \code{x.vars} -- \eqn{n x k} matrix representing \eqn{k} canonical variates \eqn{u[1]}, ..., \eqn{u[k]}.
#'   \item \code{y.coefs} -- \eqn{q x k} matrix representing \eqn{k} canonical coefficient vectors \eqn{\beta[1]}, ..., \eqn{\beta[k]}.
#'   \item \code{y.vars} -- \eqn{n x k} matrix representing \eqn{k} canonical variates \eqn{v[1]}, ..., \eqn{v[k]}.
#' }
#'
#' @examples
#' data(X)
#' data(Y)
#' #run RCCA
#' n.groups = 5
#' #run GRCCA with no sparsity on a group level
#' group1 = rep(1:n.groups, rep(ncol(X)/n.groups, n.groups))
#' grcca = GRCCA(X, Y, group1, group2 = NULL, lambda1 = 1000, lambda2 = 0, mu1 = 0, mu2 = 0)
#' #check the modified canonical correlations
#' plot(1:grcca$n.comp, grcca$mod.cors, pch = 16, xlab = 'component', 'ylab' = 'correlation', ylim = c(0, 1))
#' #check the canonical coefficients for the first canonical variates
#' barplot(grcca$x.coefs[,'can.comp1'], col = 'orange', 'xlab' = 'X feature', ylab = 'value')
#' n.groups = 50
#' #run GRCCA with sparsity on a group level
#' group1 = rep(1:n.groups, rep(ncol(X)/n.groups, n.groups))
#' grcca = GRCCA(X, Y, group1, group2 = NULL, lambda1 = 10000, lambda2 = 0, mu1 = 100, mu2 = 0)
#' #check the modified canonical correlations
#' plot(1:grcca$n.comp, grcca$mod.cors, pch = 16, xlab = 'component', 'ylab' = 'correlation', ylim = c(0, 1))
#' #check the canonical coefficients for the first canonical variates
#' barplot(grcca$x.coefs[,'can.comp1'], col = 'orange', 'xlab' = 'X feature', ylab = 'value')
#'
#' @export GRCCA
#'
GRCCA = function(X, Y, group1 = rep(1, ncol(X)), group2 = rep(1, ncol(Y)), lambda1 = 0, lambda2 = 0, mu1 = 0, mu2 = 0){
X = as.matrix(X)
Y = as.matrix(Y)
n.comp = min(ncol(X), ncol(Y), nrow(X))
#transform
X.tr = GRCCA.tr(X, group1, lambda1, mu1)
Y.tr = GRCCA.tr(Y, group2, lambda2, mu2)
if(is.null(X.tr) | is.null(Y.tr)) return(invisible(NULL))
#solve optimization problem
sol = PRCCA(X.tr$mat, Y.tr$mat, X.tr$ind, Y.tr$ind, 1, 1)
mod.cors = sol$mod.cors[1:n.comp]
#inverse transform
X.inv.tr = GRCCA.inv.tr(sol$x.coefs, group1, lambda1, mu1)
x.coefs = as.matrix(X.inv.tr$coefs[,1:n.comp])
rownames(x.coefs) = colnames(X)
x.vars = sol$x.vars[,1:n.comp]
rownames(x.vars) = rownames(X)
Y.inv.tr = GRCCA.inv.tr(sol$y.coefs, group2, lambda2, mu2)
y.coefs = as.matrix(Y.inv.tr$coefs[,1:n.comp])
rownames(y.coefs) = colnames(Y)
y.vars = sol$y.vars[,1:n.comp]
rownames(y.vars) = rownames(Y)
rho = diag(cor(x.vars,  y.vars))[1:n.comp]
names(rho) = paste('can.comp', 1:n.comp, sep = '')
return(list('n.comp' = n.comp, 'cors' = rho, 'mod.cors' = mod.cors, 'x.coefs' = x.coefs, 'x.vars' = x.vars, 'y.coefs' = y.coefs, 'y.vars' = y.vars))
}
GRCCA.tr = function(X, group, lambda, mu){
if(is.null(group)){
lambda = 0
mu = 0
}
if(lambda < 0){
cat('please make', deparse(substitute(lambda)),'> 0\n')
return(NULL)
}
if(mu < 0){
cat('please make', deparse(substitute(mu)),'> 0\n')
return(NULL)
}
index = NULL
if(lambda > 0){
#extend matrix
group.names = unique(sort(group))
ps = table(group)
agg = aggregate(t(X), by = list(group), FUN = mean)
X.mean = t(agg[, -1])
colnames(X.mean) = agg[, 1]
if(mu == 0){
mu = 1
index = 1:ncol(X)
} else {
index = 1:(ncol(X) + ncol(X.mean))
}
X1 = 1/sqrt(lambda) * (X - X.mean[,group])
X2 = scale(X.mean[,group.names], center = FALSE, scale = sqrt(mu/ps[group.names]))
X = cbind(X1, X2)
}
return(list('mat' = X, 'ind' = index))
}
GRCCA.inv.tr = function(alpha, group, lambda, mu){
if(is.null(group)){
lambda = 0
mu = 0
}
if(lambda > 0){
p = length(group)
group.names = unique(sort(group))
ps = table(group)
alpha1 = alpha[1:p,]
alpha2 = alpha[-(1:p),]
agg = aggregate(alpha1, by = list(group), FUN = mean)
alpha1.mean = agg[, -1]
rownames(alpha1.mean) = agg[, 1]
alpha1 = 1/sqrt(lambda) * (alpha1 - alpha1.mean[group,])
if(mu == 0) mu = 1
alpha2 = t(scale(t(alpha2[group.names,]), center = FALSE, scale = sqrt(mu*ps[group.names])))
alpha = alpha1 + alpha2[group,]
}
return(list('coefs' = alpha))
}
data(X)
data(Y)
#run RCCA
n.groups = 5
#run GRCCA with no sparsity on a group level
group1 = rep(1:n.groups, rep(ncol(X)/n.groups, n.groups))
grcca = GRCCA(X, Y, group1, group2 = c('a','a','a','b','b'), lambda1 = 1000, lambda2 = 1, mu1 = 1, mu2 = 1)
grcca$y.coefs
diag(cor(X %*% grcca$x.coefs, Y %*% grcca$y.coefs))
grcca$cors
grcca
#' @title Canonical Correlation analysis with L2 penalty
#'
#' @description RCCA function performs Canonical Correlation Analysis with L2 regularization and allows to conduct Canonical Correlation Analysis in high dimensions
#' For a pair of random vectors
#' \deqn{x = (x_1, \ldots, x_p)~~and~~y = (y_1, \ldots, y_q)}{x = (x_1, ..., x_p)  and  y = (y_1, ..., y_q)}
#' it seeks for such vectors
#' \deqn{\alpha = (\alpha_1, \ldots, \alpha_p)~~and~~\beta = (\beta_1, \ldots, \beta_q)}{\alpha = (\alpha_1, ..., \alpha_p)  and  \beta = (\beta_1, ..., \beta_q)}
#' that satisfy L2 constraints
#' \deqn{\|\alpha\|\leq t_1~~and~~\|\beta\| \leq t_2}{||\alpha|| <= t_1  and  ||\beta|| <= t_2}
#' and that maximize the correlation
#' \eqn{cor(u, v)}
#' between the linear combnations
#' \deqn{u = \langle x,\alpha\rangle and v = \langle y,\beta\rangle}{u = <x , \alpha> and v = <y , \beta>.}
#' Here
#' \eqn{<a , b>}
#' refers to the inner product between two vectors.
#' The optimal values for
#' \eqn{\alpha} and \eqn{\beta}
#' are called canonical coefficients and the resulting linear combinations
#' \eqn{u} and \eqn{v}
#' are called canonical variates.
#' It is actually possible to continue the process and find a sequence of canonical coefficients
#' \deqn{\alpha^1,\ldot,\alpha^k~~and~~\beta^1,\ldots\beta^k}{\alpha[1], ..., \alpha[k]  and  \beta[1], ..., \beta[k]}
#' that satisfy the L2 constraints and such that linear combinations
#' \deqn{u^i = \langle x,\alpha^i\rangle~~and~~v^i = \langle y, \beta^i\rangle}{u[i] = <x , \alpha[i]>  and  v[i] = <y , \beta[i]>}
#' form two sets
#' \deqn{\{u^1, \ldots u^k\}~~and~~\{v^1, \ldots v^k\}}{{u[1], ..., u[k]}  and  {v[k], ..., v[k]}}
#' of independent random variables. The maximmum possible number of such canonical variates is
#' \eqn{k = min(p, q)}.
#' Note that the above optimization problem is equivalet to maximizing the modified correlation coefficient
#' \deqn{\frac{cov(\langle x, \alpha\rangle, \langle y, \beta\rangle)}{\sqrt{var(\lamgle x, \alpha\rangle) + \lambda_1\|\alpha\|^2}\sqrt{var(\langle y, \beta\rangle) + \lambda_2\|\beta\|^2}}}{cov(<x , \alpha>, <y , \beta>)  /  ( cov(<x , \alpha>) + \lambda_1  ||\alpha||^2 )^1/2 ( var(<y , \beta>) + \lambda_2 ||\beta||^2 )^1/2,}
#' where \deqn{\lambda_1~~and~~\lambda_2}{\lambda_1  and  \lambda_2} control the resulting sparsity of the canonical coefficients.
#'
#' @param X a rectangular \eqn{n x p} matrix containing \eqn{n} observations of random vector \eqn{x}.
#' @param Y a rectangular \eqn{n x q} matrix containing \eqn{n} observations of random vector \eqn{y}.
#' @param lambda1 a non-negative penalty factor used for regularizing \eqn{X} side coefficients. By default \code{lambda1 = 0}, i.e. no regularization is imposed. Increasing \code{lambda1} incourages sparsity of the resulting canonical coefficients.
#' @param lambda2 a non-negative penalty factor used for regularizing \eqn{Y} side coefficients. By default \code{lambda2 = 0}, i.e. no regularization is imposed. Increasing \code{lambda2} incourages sparsity of the resulting canonical coefficients.
#' @return A list containing the PCMS problem solution:
#' \itemize{
#'   \item \code{n.comp} -- the number of computed canonical components, i.e. \eqn{k = min(p, q)}.
#'   \item \code{cors} -- the resulting \eqn{k} canonical correlations.
#'   \item \code{mod.cors} -- the resulting \eqn{k} values of modified canonical correlation.
#'   \item \code{x.coefs} -- \eqn{p x k} matrix representing \eqn{k} canonical coefficient vectors \eqn{\alpha[1]}, ..., \eqn{\alpha[k]}.
#'   \item \code{x.vars} -- \eqn{n x k} matrix representing \eqn{k} canonical variates \eqn{u[1]}, ..., \eqn{u[k]}.
#'   \item \code{y.coefs} -- \eqn{q x k} matrix representing \eqn{k} canonical coefficient vectors \eqn{\beta[1]}, ..., \eqn{\beta[k]}.
#'   \item \code{y.vars} -- \eqn{n x k} matrix representing \eqn{k} canonical variates \eqn{v[1]}, ..., \eqn{v[k]}.
#' }
#'
#' @examples
#' data(X)
#' data(Y)
#' #run RCCA
#' rcca = RCCA(X, Y, lambda1 = 10, lambda2 = 0)
#' #check the modified canonical correlations
#' plot(1:rcca$n.comp, rcca$mod.cors, pch = 16, xlab = 'component', 'ylab' = 'correlation', ylim = c(0, 1))
#' #check the canonical correlations
#' points(1:rcca$n.comp, rcca$cors, pch = 16, col = 'purple')
#' #compare them to cor(x*alpha, y*beta)
#' points(1:rcca$n.comp, diag(cor(X %*% rcca$x.coefs, Y %*% rcca$y.coefs)), col = 'cyan', pch = 16, cex = 0.7)
#' #check the canonical coefficients for the first canonical variates
#' barplot(rcca$x.coefs[,'can.comp1'], col = 'orange', 'xlab' = 'X feature', ylab = 'value')
#' barplot(rcca$y.coefs[,'can.comp1'], col = 'darkgreen', 'xlab' = 'Y feature', ylab = 'value')
#'
#' @export RCCA
RCCA = function(X, Y, lambda1 = 0, lambda2 = 0){
X = as.matrix(X)
Y = as.matrix(Y)
n.comp = min(ncol(X), ncol(Y), nrow(X))
#transform
X.tr = RCCA.tr(X, lambda1)
Y.tr = RCCA.tr(Y, lambda2)
if(is.null(X.tr) | is.null(Y.tr)) return(invisible(NULL))
#solve optimization problem
Cxx = X.tr$cor
Cyy = Y.tr$cor
Cxy = cov(X.tr$mat, Y.tr$mat, use = "pairwise")
sol = fda::geigen(Cxy, Cxx, Cyy)
names(sol) = c("rho", "alpha", "beta")
#modified canonical correlation
rho.mod = sol$rho[1:n.comp]
names(rho.mod) = paste('can.comp', 1:n.comp, sep = '')
#inverse transform
X.inv.tr = RCCA.inv.tr(X.tr$mat, sol$alpha, X.tr$tr)
x.coefs = as.matrix(X.inv.tr$coefs[,1:n.comp])
rownames(x.coefs) = colnames(X)
x.vars = X.inv.tr$vars[,1:n.comp]
rownames(x.vars) = rownames(X)
Y.inv.tr = RCCA.inv.tr(Y.tr$mat, sol$beta, Y.tr$tr)
y.coefs = as.matrix(Y.inv.tr$coefs[,1:n.comp])
rownames(y.coefs) = colnames(Y)
y.vars = Y.inv.tr$vars[,1:n.comp]
rownames(y.vars) = rownames(Y)
#canonical correlation
rho = diag(cor(X.inv.tr$vars,  Y.inv.tr$vars))[1:n.comp]
names(rho) = paste('can.comp', 1:n.comp, sep = '')
return(list('n.comp' = n.comp, 'cors' = rho, 'mod.cors' = rho.mod, 'x.coefs' = x.coefs, 'x.vars' = x.vars, 'y.coefs' = y.coefs, 'y.vars' = y.vars))
}
RCCA.tr = function(X, lambda){
p = ncol(X)
n = nrow(X)
if(lambda < 0){
cat('please make', deparse(substitute(lambda)),'>= 0\n')
return()
}
#kernel trick
V = NULL
if(p > n){
if(lambda == 0){
cat('singularity issue. please impose a penalty on', deparse(substitute(X)), 'side\n')
return()
}
SVD = svd(X)
X = SVD$u %*% diag(SVD$d)
V = SVD$v
}
C = var(X, use = "pairwise")
diag(C) = diag(C) + lambda
return(list('mat' = X, 'tr' = V, 'cor' = C))
}
RCCA.inv.tr = function(X, alpha, V){
n.comp = ncol(alpha)
#find canonical variates
u = X %*% alpha
colnames(u) = paste('can.comp', 1:n.comp, sep = '')
#inverse transfrom canonical coefficients
if(!is.null(V)) alpha = V %*% alpha
colnames(alpha) = paste('can.comp', 1:n.comp, sep = '')
return(list('coefs' = alpha, 'vars' = u))
}
#' @title Canonical Correlation analysis with partial L2 penalty
#'
#' @description PRCCA function performs Canonical Correlation Analysis with partial L2 regularization and allows to conduct Canonical Correlation Analysis in high dimensions.
#' It imposes L2 penalty only on a subset of \eqn{\alpha} and \eqn{\beta} coefficients. Specifically, if
#' \deqn{x = (x_1, \ldots, x_p)~~and~~y = (y_1, \ldots, y_q)}{x = (x_1, ..., x_p)  and  y = (y_1, ..., y_q)}
#' are random vectors and
#' \deqn{I = \{i_1, ..., i_m\} \subset \{1, ..., p\}~~and~~J = {j_1, ..., i_r} \subset \{1, ..., q\}}{I = {i_1, ..., i_m}  is a subset of  {1, ..., p}  and  J = {j_1, ..., i_r}  is a subset of  {1, ..., q}}
#' then
#' PRCCA seeks for such vectors
#' \deqn{\alpha = (\alpha_1, \ldots, \alpha_p)~~and~~\beta = (\beta_1, \ldots, \beta_q)}{\alpha = (\alpha_1, ..., \alpha_p)  and  \beta = (\beta_1, ..., \beta_q)}
#' that satisfy partial L2 constraints
#' \deqn{\|\alpha_I\|\leq t_1~~and~~\|\beta_J\| \leq t_2}{||\alpha_I|| <= t_1  and  ||\beta_J|| <= t_2}
#' and that maximize the correlation
#' \eqn{cor(u, v)}
#' between the linear combnations
#' \deqn{u = \langle x,\alpha\rangle and v = \langle y,\beta\rangle}{u = <x , \alpha> and v = <y , \beta>.}
#' Here
#' \eqn{<a , b>}
#' refers to the inner product between two vectors and
#' \deqn{\alpha_I~~and~~\beta_J}{\alpha_I  and  \beta_J}
#' are corresponding subvectors of \eqn{\alpha} and \eqn{\beta} with indices belonging to \eqn{I} and \eqn{J}, respectively.
#' Again, the above optimization problem is equivalet to maximizing the modified correlation coefficient
#' \deqn{\frac{cov(\langle x, \alpha\rangle, \langle y, \beta\rangle)}{\sqrt{var(\lamgle x, \alpha\rangle) + \lambda_1\|\alpha_I\|^2}\sqrt{var(\langle y, \beta\rangle) + \lambda_2\|\beta_J\|^2}}}{cov(<x , \alpha>, <y , \beta>)  /  ( cov(<x , \alpha>) + \lambda_1  ||\alpha_I||^2 )^1/2 ( var(<y , \beta>) + \lambda_2 ||\beta_J||^2 )^1/2,}
#' where \deqn{\lambda_1~~and~~\lambda_2}{\lambda_1  and  \lambda_2} control the resulting sparsity of the canonical coefficients within \deqn{\alpha_I~~and~~\beta_J}{\alpha_I  and  \beta_J} parts of the coefficient vectors.
#'
#'
#' @param X a rectangular \eqn{n x p} matrix containing \eqn{n} observations of random vector \eqn{x}.
#' @param Y a rectangular \eqn{n x q} matrix containing \eqn{n} observations of random vector \eqn{y}.
#' @param index1 a subset of indices the penalty is imposed on while regularizing the \eqn{X} side. By default \code{index1 = 1:ncol(X)}, i.e. we include all \eqn{\alpha} coefficients in the relularization term.
#' @param index2 a subset of indices the penalty is imposed on while regularizing the \eqn{Y} side. By default \code{index2 = 1:ncol(Y)}, i.e. we include all \eqn{\beta} coefficients in the relularization term.
#' @param lambda1 a non-negative penalty factor used for regularizing \eqn{X} side coefficients \eqn{\alpha}. By default \code{lambda1 = 0}, i.e. no regularization is imposed. Increasing \code{lambda1} incourages sparsity of the resulting canonical coefficients.
#' @param lambda2 a non-negative penalty factor used for regularizing \eqn{Y} side coefficients \eqn{\beta}. By default \code{lambda2 = 0}, i.e. no regularization is imposed. Increasing \code{lambda2} incourages sparsity of the resulting canonical coefficients.
#' @return A list containing the PCMS problem solution:
#' \itemize{
#'   \item \code{n.comp} -- the number of computed canonical components, i.e. \eqn{k = min(p, q)}.
#'   \item \code{cors} -- the resulting \eqn{k} canonical correlations.
#'   \item \code{mod.cors} -- the resulting \eqn{k} values of modified canonical correlation.
#'   \item \code{x.coefs} -- \eqn{p x k} matrix representing \eqn{k} canonical coefficient vectors \eqn{\alpha[1]}, ..., \eqn{\alpha[k]}.
#'   \item \code{x.vars} -- \eqn{n x k} matrix representing \eqn{k} canonical variates \eqn{u[1]}, ..., \eqn{u[k]}.
#'   \item \code{y.coefs} -- \eqn{q x k} matrix representing \eqn{k} canonical coefficient vectors \eqn{\beta[1]}, ..., \eqn{\beta[k]}.
#'   \item \code{y.vars} -- \eqn{n x k} matrix representing \eqn{k} canonical variates \eqn{v[1]}, ..., \eqn{v[k]}.
#' }
#'
#' @examples
#' data(X)
#' data(Y)
#' #run RCCA
#' prcca = PRCCA(X, Y, lambda1 = 100, lambda2 = 0, index1 = 1:(ncol(X) - 10))
#' #check the modified canonical correlations
#' plot(1:prcca$n.comp, prcca$mod.cors, pch = 16, xlab = 'component', 'ylab' = 'correlation', ylim = c(0, 1))
#' #check the canonical correlations
#' points(1:prcca$n.comp, prcca$cors, pch = 16, col = 'purple')
#' #compare them to cor(x*alpha, y*beta)
#' points(1:prcca$n.comp, diag(cor(X %*% prcca$x.coefs, Y %*% prcca$y.coefs)), col = 'cyan', pch = 16, cex = 0.7)
#' #check the canonical coefficients for the first canonical variates
#' barplot(prcca$x.coefs[,'can.comp1'], col = 'orange', 'xlab' = 'X feature', ylab = 'value')
#' barplot(prcca$y.coefs[,'can.comp1'], col = 'darkgreen', 'xlab' = 'Y feature', ylab = 'value')
#'
#' @export PRCCA
PRCCA = function(X, Y, index1 = 1:ncol(X), index2 = 1:ncol(Y), lambda1 = 0, lambda2 = 0){
X = as.matrix(X)
Y = as.matrix(Y)
n.comp = min(ncol(X), ncol(Y), nrow(X))
#transform
X.tr = PRCCA.tr(X, index1, lambda1)
Y.tr = PRCCA.tr(Y, index2, lambda2)
if(is.null(X.tr) | is.null(Y.tr)) return(invisible(NULL))
#solve optimization problem
Cxx = X.tr$cor
Cyy = Y.tr$cor
Cxy = cov(X.tr$mat, Y.tr$mat, use = "pairwise")
sol = fda::geigen(Cxy, Cxx, Cyy)
names(sol) = c("rho", "alpha", "beta")
#find modified correlation
rho.mod = sol$rho[1:n.comp]
names(rho.mod) = paste('can.comp', 1:n.comp, sep = '')
#inverse transform
X.inv.tr = PRCCA.inv.tr(X.tr$mat, X.tr$p.pen, sol$alpha, X.tr$tr)
x.coefs = as.matrix(X.inv.tr$coefs[,1:n.comp])
rownames(x.coefs) = colnames(X)
x.vars = X.inv.tr$vars[,1:n.comp]
rownames(x.vars) = rownames(X)
Y.inv.tr = PRCCA.inv.tr(Y.tr$mat, Y.tr$p.pen, sol$beta, Y.tr$tr)
y.coefs = as.matrix(Y.inv.tr$coefs[,1:n.comp])
rownames(y.coefs) = colnames(Y)
y.vars = Y.inv.tr$vars[,1:n.comp]
rownames(y.vars) = rownames(Y)
#canonical correlation
rho = diag(cor(X.inv.tr$vars,  Y.inv.tr$vars))[1:n.comp]
names(rho) = paste('can.comp', 1:n.comp, sep = '')
return(list('n.comp' = n.comp, 'cors' = rho, 'mod.cors' = rho.mod, 'x.coefs' = x.coefs, 'x.vars' = x.vars, 'y.coefs' = y.coefs, 'y.vars' = y.vars))
}
PRCCA.tr = function(X, index, lambda){
p = ncol(X)
n = nrow(X)
if(lambda < 0){
cat('please make', deparse(substitute(lambda)),'>= 0\n')
return()
}
if(is.null(index)){
lambda = 0
index = 1:p
}
if(lambda == 0 & p > n){
cat('singularity issue. please impose a penalty on', deparse(substitute(X)), 'side\n')
return()
}
penalty = rep(0, p)
permute = NULL
B = NULL
V = NULL
p1 = length(index)
if(lambda > 0){
X1 = X[, index, drop = FALSE]
X2 = X[, -index, drop = FALSE]
permute = order(c(index, (1:p)[-index]))
if(ncol(X2) >= n){
cat('singularity issue. please penalize more',deparse(substitute(X)) ,'features\n')
return()
}
if(ncol(X2) > 0){
#regress X2 from X1
B = solve(t(X2) %*% X2) %*% t(X2) %*% X1
X1 = X1 - X2 %*% B
}
if(ncol(X1) > n){
#apply kernel trick
SVD = svd(X1)
X1 = SVD$u %*% diag(SVD$d)
V = SVD$v
p1 = ncol(X1)
}
p1 = ncol(X1)
X = cbind(X1, X2)
penalty = rep(0, ncol(X))
penalty[1:p1] = lambda
}
C = var(X, use = "pairwise")
diag(C) = diag(C) + penalty
return(list('mat' = X, 'p.pen' = p1, 'tr' = list('reg' = B, 'ker' = V, 'perm' = permute), 'cor' = C))
}
PRCCA.inv.tr = function(X, p1, alpha, tr){
n.comp = ncol(alpha)
colnames(alpha) = paste('can.comp', 1:n.comp, sep = '')
V = tr$ker
B = tr$reg
permute = tr$perm
#find canonical variates
u = X %*% alpha
colnames(u) = paste('can.comp', 1:n.comp, sep = '')
#inverse transfrom canonical coefficients
if(!is.null(p1)){
alpha1 = alpha[1:p1, , drop = FALSE]
alpha2 = alpha[-(1:p1), , drop = FALSE]
if(!is.null(V)) alpha1 = V %*% alpha1
if(!is.null(B)) alpha2 = -B %*% alpha1 + alpha2
alpha = rbind(alpha1, alpha2)
if(!is.null(permute)) alpha = alpha[permute, ]
}
return(list('coefs' = alpha, 'vars' = u))
}
data(X)
data(Y)
#run RCCA
n.groups = 5
#run GRCCA with no sparsity on a group level
group1 = rep(1:n.groups, rep(ncol(X)/n.groups, n.groups))
grcca = GRCCA(X, Y, group1, group2 = c('a','a','a','b','b'), lambda1 = 1000, lambda2 = 1, mu1 = 1, mu2 = 1)
grcca
X
colnames(X) = paste('f', 1:ncol(X))
rownames(X = paste('s', 1:nrow(X))
)
rownames(X) = paste('s', 1:nrow(X))
X
colnames(Y) = paste('f', 1:ncol(Y))
rownames(Y) = paste('s', 1:nrow(Y))
Y
data(X)
data(Y)
#run RCCA
n.groups = 5
#run GRCCA with no sparsity on a group level
group1 = rep(1:n.groups, rep(ncol(X)/n.groups, n.groups))
grcca = GRCCA(X, Y, group1, group2 = c('a','a','a','b','b'), lambda1 = 1000, lambda2 = 1, mu1 = 1, mu2 = 1)
grcca
grcca$x.coefs
dim(grcca$x.coefs)
rownames(grcca$x.coefs) = paste('s', 1:nrow(X))
rownames(grcca$x.coefs) = paste('s', 1:ncol(X))
grcca$x.coefs
colnames(grcca$x.coefs) = NULL
rownames(X)
x
X
rownames(Y) = paste('s', 1:nrow(Y))
colnames(Y) = paste('f', 1:ncol(Y))
rownames(X) = paste('s', 1:nrow(X))
colnames(X) = paste('f', 1:ncol(X))
grcca = GRCCA(X, Y, group1, group2 = c('a','a','a','b','b'), lambda1 = 1000, lambda2 = 1, mu1 = 1, mu2 = 1)
grcca
diag(cor(X %*% grcca$x.coefs, Y %*% grcca$y.coefs))
grcca$cors
X %*% grcca$x.coefs / grcca$x.coefs
X %*% grcca$x.coefs / grcca$x.vars
grcca = GRCCA(X, Y, group1, group2 = c('a','a','a','b','b'), lambda1 = 100000, lambda2 = 100000, mu1 = 1, mu2 = 1)
grcca$x.coefs[,1]
plot(grcca$x.coefs[,1])
plot(grcca$y.coefs[,1])
grcca = GRCCA(X, Y, group1, group2 = c('a','a','a','b','b'), lambda1 = 100000, lambda2 = 100000, mu1 = 10000000, mu2 = 1000000)
plot(grcca$x.coefs[,1])
plot(grcca$y.coefs[,1])
grcca = GRCCA(X, Y, group1, group2 = c('a','a','a','b','b'), lambda1 = 100000, lambda2 = 100000, mu1 = 10000000, mu2 = 10000000000)
plot(grcca$y.coefs[,1])
plot(grcca$y.coefs[,1])
grcca = GRCCA(X, Y, group1, group2 = c('a','a','a','b','b'), lambda1 = 100000, lambda2 = 100000, mu1 = 10000000, mu2 = 10000000000)
plot(grcca$y.coefs[,1])
grcca = GRCCA(X, Y, group1, group2 = c('a','a','a','b','b'), lambda1 = 100000, lambda2 = 100000, mu1 = 10000000, mu2 = 10000000000000000)
plot(grcca$y.coefs[,1])
grcca = GRCCA(X, Y, group1, group2 = c('a','a','a','b','b'), lambda1 = 100000, lambda2 = 100000, mu1 = 10000000000, mu2 = 10000000000000000)
plot(grcca$y.coefs[,1])
grcca = GRCCA(X, Y, group1, group2 = c('a','a','a','b','b'), lambda1 = 100000, lambda2 = 100000, mu1 = 100, mu2 = 1000)
plot(grcca$y.coefs[,1])
grcca = GRCCA(X, Y, group1, group2 = c('a','a','a','b','b'), lambda1 = 100000, lambda2 = 100000, mu1 = 1000, mu2 = 1000)
plot(grcca$y.coefs[,1])
grcca = GRCCA(X, Y, group1, group2 = c('a','a','a','b','b'), lambda1 = 100000, lambda2 = 100000, mu1 = 1000, mu2 = 100000)
plot(grcca$y.coefs[,1])
grcca = GRCCA(X, Y, group1, group2 = c('a','a','a','b','b'), lambda1 = 1000, lambda2 = 1000, mu1 = 1000, mu2 = 1000)
grcca$cors
rcca = RCCA(X, Y, lambda1 = 1000, lambda2 = 1000)
rcca$cors
rcca$x.coefs/grcca$x.coefs
rcca$x.coefs/grcca$y.coefs
rcca$y.coefs/grcca$y.coefs
grcca = GRCCA(X, Y, group1, group2 = c('a','a','a','b','b'), lambda1 = 1000, lambda2 = 100000000, mu1 = 1000, mu2 = 1000)
plot(grcca$x.coefs[,1])
grcca = GRCCA(X, Y, group1, group2 = c('a','a','a','b','b'), lambda1 = 1000, lambda2 = 100000000000, mu1 = 1000, mu2 = 1000)
plot(grcca$x.coefs[,1])
grcca$x.coefs[,1]
grcca = GRCCA(X, Y, group1, group2 = c('a','a','a','b','b'), lambda1 = 1000000, lambda2 = 100000000000, mu1 = 1000, mu2 = 1000)
grcca$x.coefs[,1]
plot(grcca$x.coefs[,1])
remove.packages(RCCA)
remove.packages('RCCA')
RCCA::RCCA
?RCCA::RCCA
?RCCA:RCCA
